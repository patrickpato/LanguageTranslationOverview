{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vKKD2bzWRf31",
        "outputId": "0d872fe7-60bc-491a-c820-126f09319ef9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WUiZSmd-nw8S",
        "outputId": "4ad5c765-96a5-411d-ee21-b7d6e5b2c860"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "E: Unable to locate package python-pil\n",
            "E: Unable to locate package python-lxml\n"
          ]
        }
      ],
      "source": [
        "!apt-get -y install protobuf-compiler python-pil python-lxml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uLEFcu7tn5du",
        "outputId": "9d59aee1-700c-4505-d29d-90460ef67061"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openNMT-py\n",
            "  Downloading OpenNMT_py-3.5.1-py3-none-any.whl.metadata (8.8 kB)\n",
            "Collecting torch<2.3,>=2.1 (from openNMT-py)\n",
            "  Downloading torch-2.2.2-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\n",
            "Collecting configargparse (from openNMT-py)\n",
            "  Downloading ConfigArgParse-1.7-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting ctranslate2<5,>=4 (from openNMT-py)\n",
            "  Downloading ctranslate2-4.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: tensorboard>=2.3 in /usr/local/lib/python3.10/dist-packages (from openNMT-py) (2.17.0)\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.10/dist-packages (from openNMT-py) (2.2.5)\n",
            "Collecting waitress (from openNMT-py)\n",
            "  Downloading waitress-3.0.0-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting pyonmttok<2,>=1.37 (from openNMT-py)\n",
            "  Downloading pyonmttok-1.37.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from openNMT-py) (6.0.2)\n",
            "Collecting sacrebleu (from openNMT-py)\n",
            "  Downloading sacrebleu-2.4.3-py3-none-any.whl.metadata (51 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rapidfuzz (from openNMT-py)\n",
            "  Downloading rapidfuzz-3.9.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting pyahocorasick (from openNMT-py)\n",
            "  Downloading pyahocorasick-2.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (13 kB)\n",
            "Collecting fasttext-wheel (from openNMT-py)\n",
            "  Downloading fasttext_wheel-0.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (from openNMT-py) (3.7.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from openNMT-py) (1.16.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from ctranslate2<5,>=4->openNMT-py) (71.0.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from ctranslate2<5,>=4->openNMT-py) (1.26.4)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->openNMT-py) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->openNMT-py) (1.64.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->openNMT-py) (3.7)\n",
            "Requirement already satisfied: protobuf!=4.24.0,<5.0.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->openNMT-py) (3.20.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->openNMT-py) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->openNMT-py) (3.0.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<2.3,>=2.1->openNMT-py) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch<2.3,>=2.1->openNMT-py) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch<2.3,>=2.1->openNMT-py) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<2.3,>=2.1->openNMT-py) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<2.3,>=2.1->openNMT-py) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch<2.3,>=2.1->openNMT-py) (2024.6.1)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch<2.3,>=2.1->openNMT-py)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch<2.3,>=2.1->openNMT-py)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch<2.3,>=2.1->openNMT-py)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch<2.3,>=2.1->openNMT-py)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch<2.3,>=2.1->openNMT-py)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch<2.3,>=2.1->openNMT-py)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch<2.3,>=2.1->openNMT-py)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch<2.3,>=2.1->openNMT-py)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch<2.3,>=2.1->openNMT-py)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch<2.3,>=2.1->openNMT-py)\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch<2.3,>=2.1->openNMT-py)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==2.2.0 (from torch<2.3,>=2.1->openNMT-py)\n",
            "  Downloading triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch<2.3,>=2.1->openNMT-py)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.6.68-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting pybind11>=2.2 (from fasttext-wheel->openNMT-py)\n",
            "  Downloading pybind11-2.13.5-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from flask->openNMT-py) (2.2.0)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from flask->openNMT-py) (8.1.7)\n",
            "Collecting portalocker (from sacrebleu->openNMT-py)\n",
            "  Downloading portalocker-2.10.1-py3-none-any.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu->openNMT-py) (2024.5.15)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu->openNMT-py) (0.9.0)\n",
            "Collecting colorama (from sacrebleu->openNMT-py)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu->openNMT-py) (4.9.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy->openNMT-py) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy->openNMT-py) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy->openNMT-py) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy->openNMT-py) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy->openNMT-py) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy->openNMT-py) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy->openNMT-py) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy->openNMT-py) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy->openNMT-py) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy->openNMT-py) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy->openNMT-py) (0.12.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy->openNMT-py) (4.66.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy->openNMT-py) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy->openNMT-py) (2.8.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy->openNMT-py) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy->openNMT-py) (3.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<2.3,>=2.1->openNMT-py) (2.1.5)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy->openNMT-py) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->openNMT-py) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->openNMT-py) (2.20.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy->openNMT-py) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy->openNMT-py) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy->openNMT-py) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy->openNMT-py) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy->openNMT-py) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy->openNMT-py) (0.1.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy->openNMT-py) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy->openNMT-py) (13.8.0)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy->openNMT-py) (0.19.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy->openNMT-py) (7.0.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch<2.3,>=2.1->openNMT-py) (1.3.0)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy->openNMT-py) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy->openNMT-py) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy->openNMT-py) (2.16.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy->openNMT-py) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy->openNMT-py) (0.1.2)\n",
            "Downloading OpenNMT_py-3.5.1-py3-none-any.whl (262 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m262.8/262.8 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ctranslate2-4.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.2/37.2 MB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyonmttok-1.37.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.0/17.0 MB\u001b[0m \u001b[31m80.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch-2.2.2-cp310-cp310-manylinux1_x86_64.whl (755.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.5/755.5 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m106.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m85.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m52.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ConfigArgParse-1.7-py3-none-any.whl (25 kB)\n",
            "Downloading fasttext_wheel-0.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m101.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyahocorasick-2.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (110 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.7/110.7 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rapidfuzz-3.9.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m90.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sacrebleu-2.4.3-py3-none-any.whl (103 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.0/104.0 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading waitress-3.0.0-py3-none-any.whl (56 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.7/56.7 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pybind11-2.13.5-py3-none-any.whl (240 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.0/241.0 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading portalocker-2.10.1-py3-none-any.whl (18 kB)\n",
            "Downloading nvidia_nvjitlink_cu12-12.6.68-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m78.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: waitress, triton, rapidfuzz, pyonmttok, pybind11, pyahocorasick, portalocker, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, ctranslate2, configargparse, colorama, sacrebleu, nvidia-cusparse-cu12, nvidia-cudnn-cu12, fasttext-wheel, nvidia-cusolver-cu12, torch, openNMT-py\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.22.3\n",
            "    Uninstalling nvidia-nccl-cu12-2.22.3:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.22.3\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.4.0+cu121\n",
            "    Uninstalling torch-2.4.0+cu121:\n",
            "      Successfully uninstalled torch-2.4.0+cu121\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.4.0+cu121 requires torch==2.4.0, but you have torch 2.2.2 which is incompatible.\n",
            "torchvision 0.19.0+cu121 requires torch==2.4.0, but you have torch 2.2.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed colorama-0.4.6 configargparse-1.7 ctranslate2-4.4.0 fasttext-wheel-0.9.2 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.6.68 nvidia-nvtx-cu12-12.1.105 openNMT-py-3.5.1 portalocker-2.10.1 pyahocorasick-2.1.0 pybind11-2.13.5 pyonmttok-1.37.1 rapidfuzz-3.9.7 sacrebleu-2.4.3 torch-2.2.2 triton-2.2.0 waitress-3.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install openNMT-py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3BPgqwFl9PH"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E-sDfaoRoKC8"
      },
      "outputs": [],
      "source": [
        "config = '''# config.yaml\n",
        "\n",
        "\n",
        "## Where the samples will be written\n",
        "save_data: run\n",
        "\n",
        "# Training files\n",
        "data:\n",
        "    corpus_1:\n",
        "        path_src: /content/drive/MyDrive/EngSwaBBC/cleanGoURMETEng.txt.subword.train\n",
        "        path_tgt: /content/drive/MyDrive/EngSwaBBC/cleanGoURMETSwa.txt.subword.train\n",
        "        transforms: [filtertoolong]\n",
        "    valid:\n",
        "        path_src: /content/drive/MyDrive/EngSwaBBC/cleanGoURMETEng.txt.subword.dev\n",
        "        path_tgt: /content/drive/MyDrive/EngSwaBBC/cleanGoURMETSwa.txt.subword.dev\n",
        "        transforms: [filtertoolong]\n",
        "\n",
        "# Vocabulary files, generated by onmt_build_vocab\n",
        "src_vocab: run/source.vocab\n",
        "tgt_vocab: run/target.vocab\n",
        "\n",
        "# Vocabulary size - should be the same as in sentence piece\n",
        "src_vocab_size: 50000\n",
        "tgt_vocab_size: 50000\n",
        "\n",
        "# Filter out source/target longer than n if [filtertoolong] enabled\n",
        "src_seq_length: 64\n",
        "tgt_seq_length: 64\n",
        "\n",
        "# Tokenization options\n",
        "src_subword_model: /content/drive/MyDrive/EngSwaBBC/source.model\n",
        "tgt_subword_model: /content/drive/MyDrive/EngSwaBBC/target.model\n",
        "# Where to save the log file and the output models/checkpoints\n",
        "log_file: train.log\n",
        "save_model: models/model.engswa\n",
        "\n",
        "# Stop training if it does not imporve after n validations\n",
        "early_stopping: 10\n",
        "\n",
        "# Default: 5000 - Save a model checkpoint for each n\n",
        "save_checkpoint_steps: 1000\n",
        "\n",
        "# To save space, limit checkpoints to last n\n",
        "# keep_checkpoint: 3\n",
        "\n",
        "seed: 2023\n",
        "\n",
        "# Default: 100000 - Train the model to max n steps\n",
        "# Increase to 200000 or more for large datasets\n",
        "# For fine-tuning, add up the required steps to the original steps\n",
        "train_steps: 15000\n",
        "\n",
        "# Default: 10000 - Run validation after n steps\n",
        "valid_steps: 4000\n",
        "\n",
        "# Default: 4000 - for large datasets, try up to 8000\n",
        "warmup_steps: 5000\n",
        "report_every: 100\n",
        "\n",
        "# Number of GPUs, and IDs of GPUs\n",
        "world_size: 1\n",
        "gpu_ranks: [0]\n",
        "\n",
        "# Batching\n",
        "bucket_size: 262144\n",
        "num_workers: 0  # Default: 2, set to 0 when RAM out of memory\n",
        "batch_type: \"tokens\"\n",
        "batch_size: 1024  # Tokens per batch, change when CUDA out of memory\n",
        "valid_batch_size: 1024\n",
        "max_generator_batches: 2\n",
        "accum_count: [4]\n",
        "accum_steps: [0]\n",
        "\n",
        "# Optimization\n",
        "model_dtype: \"fp16\"\n",
        "optim: \"adam\"\n",
        "learning_rate: 0.4\n",
        "# warmup_steps: 8000\n",
        "decay_method: \"noam\"\n",
        "adam_beta2: 0.998\n",
        "max_grad_norm: 0\n",
        "label_smoothing: 0.1\n",
        "param_init: 0\n",
        "param_init_glorot: true\n",
        "normalization: \"tokens\"\n",
        "\n",
        "# Model\n",
        "encoder_type: transformer\n",
        "decoder_type: transformer\n",
        "position_encoding: true\n",
        "enc_layers: 16\n",
        "dec_layers: 16\n",
        "heads: 16\n",
        "hidden_size: 1024\n",
        "word_vec_size: 1024\n",
        "transformer_ff: 2048\n",
        "dropout_steps: [0]\n",
        "dropout: [0.1]\n",
        "attention_dropout: [0.1]\n",
        "'''\n",
        "\n",
        "with open(\"config.yaml\", \"w+\") as config_yaml:\n",
        "  config_yaml.write(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2lNRjFUyFMl9",
        "outputId": "4c4af1ad-8759-4425-c20e-8849de0f4671"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corpus corpus_1's weight should be given. We default it to 1 for you.\n",
            "[2024-09-10 02:21:42,999 INFO] Counter vocab from -1 samples.\n",
            "[2024-09-10 02:21:42,999 INFO] n_sample=-1: Build vocab on full datasets.\n",
            "[2024-09-10 02:21:51,993 INFO] * Transform statistics for corpus_1(50.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=14301)\n",
            "\n",
            "[2024-09-10 02:21:52,034 INFO] * Transform statistics for corpus_1(50.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=14294)\n",
            "\n",
            "[2024-09-10 02:21:52,085 INFO] Counters src: 14005\n",
            "[2024-09-10 02:21:52,086 INFO] Counters tgt: 16122\n"
          ]
        }
      ],
      "source": [
        "!onmt_build_vocab -config /content/config.yaml -n_sample -1 -num_threads 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OFY_xrILFTPL",
        "outputId": "4af2e7ce-5e66-4e6b-b042-f1be89047f9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2024-09-10 02:21:57,970 WARNING] Corpus corpus_1's weight should be given. We default it to 1 for you.\n",
            "[2024-09-10 02:21:57,971 INFO] Parsed 2 corpora from -data.\n",
            "[2024-09-10 02:21:57,971 INFO] Get special vocabs from Transforms: {'src': [], 'tgt': []}.\n",
            "[2024-09-10 02:21:58,072 INFO] The first 10 tokens of the vocabs are:['<unk>', '<blank>', '<s>', '</s>', '.', '▁the', ',', '▁to', '▁of', '▁and']\n",
            "[2024-09-10 02:21:58,072 INFO] The decoder start token is: <s>\n",
            "[2024-09-10 02:21:58,072 INFO] Building model...\n",
            "[2024-09-10 02:22:05,663 INFO] Switching model to float32 for amp/apex_amp\n",
            "[2024-09-10 02:22:05,663 INFO] Non quantized layer compute is fp16\n",
            "[2024-09-10 02:22:06,390 INFO] NMTModel(\n",
            "  (encoder): TransformerEncoder(\n",
            "    (embeddings): Embeddings(\n",
            "      (make_embedding): Sequential(\n",
            "        (emb_luts): Elementwise(\n",
            "          (0): Embedding(14016, 1024, padding_idx=1)\n",
            "        )\n",
            "        (pe): PositionalEncoding()\n",
            "      )\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (transformer): ModuleList(\n",
            "      (0-15): 16 x TransformerEncoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "          (linear_values): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "          (linear_query): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=1024, out_features=2048, bias=False)\n",
            "          (w_2): Linear(in_features=2048, out_features=1024, bias=False)\n",
            "          (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
            "  )\n",
            "  (decoder): TransformerDecoder(\n",
            "    (embeddings): Embeddings(\n",
            "      (make_embedding): Sequential(\n",
            "        (emb_luts): Elementwise(\n",
            "          (0): Embedding(16128, 1024, padding_idx=1)\n",
            "        )\n",
            "        (pe): PositionalEncoding()\n",
            "      )\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
            "    (transformer_layers): ModuleList(\n",
            "      (0-15): 16 x TransformerDecoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "          (linear_values): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "          (linear_query): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=1024, out_features=2048, bias=False)\n",
            "          (w_2): Linear(in_features=2048, out_features=1024, bias=False)\n",
            "          (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (layer_norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (context_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "          (linear_values): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "          (linear_query): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "        )\n",
            "        (layer_norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (generator): Linear(in_features=1024, out_features=16128, bias=True)\n",
            ")\n",
            "[2024-09-10 02:22:06,399 INFO] encoder: 148637696\n",
            "[2024-09-10 02:22:06,399 INFO] decoder: 234473216\n",
            "[2024-09-10 02:22:06,399 INFO] * number of parameters: 383110912\n",
            "[2024-09-10 02:22:06,402 INFO] Trainable parameters = {'torch.float32': 383110912, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}\n",
            "[2024-09-10 02:22:06,403 INFO] Non trainable parameters = {'torch.float32': 0, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}\n",
            "[2024-09-10 02:22:06,403 INFO]  * src vocab size = 14016\n",
            "[2024-09-10 02:22:06,403 INFO]  * tgt vocab size = 16128\n",
            "[2024-09-10 02:22:06,893 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 1\n",
            "[2024-09-10 02:22:06,894 INFO] Starting training on GPU: [0]\n",
            "[2024-09-10 02:22:06,894 INFO] Start training loop and validate every 4000 steps...\n",
            "[2024-09-10 02:22:06,894 INFO] Scoring with: ['filtertoolong']\n",
            "[2024-09-10 02:22:16,214 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 2\n",
            "[2024-09-10 02:22:26,326 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 3\n",
            "[2024-09-10 02:24:07,588 INFO] Step 100/15000; acc: 2.5; ppl: 10810.2; xent: 9.3; lr: 0.00000; sents:   11048; bsz:  765/ 963/28; 2537/3192 tok/s;    121 sec;\n",
            "[2024-09-10 02:25:37,270 INFO] Step 200/15000; acc: 3.8; ppl: 5088.8; xent: 8.5; lr: 0.00001; sents:   10943; bsz:  771/ 964/27; 3437/4299 tok/s;    210 sec;\n",
            "[2024-09-10 02:27:06,726 INFO] Step 300/15000; acc: 5.7; ppl: 2273.5; xent: 7.7; lr: 0.00001; sents:   10860; bsz:  776/ 966/27; 3472/4321 tok/s;    300 sec;\n",
            "[2024-09-10 02:28:36,080 INFO] Step 400/15000; acc: 6.9; ppl: 1102.8; xent: 7.0; lr: 0.00001; sents:   10710; bsz:  767/ 960/27; 3432/4299 tok/s;    389 sec;\n",
            "[2024-09-10 02:30:05,445 INFO] Step 500/15000; acc: 7.7; ppl: 830.2; xent: 6.7; lr: 0.00002; sents:   11232; bsz:  763/ 961/28; 3414/4303 tok/s;    479 sec;\n",
            "[2024-09-10 02:31:34,913 INFO] Step 600/15000; acc: 9.6; ppl: 706.1; xent: 6.6; lr: 0.00002; sents:   11093; bsz:  774/ 966/28; 3460/4320 tok/s;    568 sec;\n",
            "[2024-09-10 02:33:04,346 INFO] Step 700/15000; acc: 14.9; ppl: 460.1; xent: 6.1; lr: 0.00002; sents:   10913; bsz:  763/ 964/27; 3411/4313 tok/s;    657 sec;\n",
            "[2024-09-10 02:34:33,811 INFO] Step 800/15000; acc: 19.3; ppl: 291.6; xent: 5.7; lr: 0.00003; sents:   11086; bsz:  768/ 964/28; 3435/4310 tok/s;    747 sec;\n",
            "[2024-09-10 02:36:03,333 INFO] Step 900/15000; acc: 21.4; ppl: 227.9; xent: 5.4; lr: 0.00003; sents:   11162; bsz:  769/ 961/28; 3437/4295 tok/s;    836 sec;\n",
            "[2024-09-10 02:37:33,059 INFO] Step 1000/15000; acc: 23.5; ppl: 183.2; xent: 5.2; lr: 0.00004; sents:   10876; bsz:  776/ 964/27; 3460/4297 tok/s;    926 sec;\n",
            "[2024-09-10 02:37:33,068 INFO] Saving checkpoint models/model.engswa_step_1000.pt\n"
          ]
        }
      ],
      "source": [
        " # Train the NMT model\n",
        "!onmt_train -config /content/config.yaml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fx9_vrq0gC6a"
      },
      "outputs": [],
      "source": [
        "!onmt_translate -model /content/models/model.engswa_step_15000.pt -src /content/drive/MyDrive/EngSwaBBC/cleanGoURMETEng.txt.subword.test -output /content/drive/MyDrive/EngSwaBBC/GoURMETSwa_Translated_BestModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9T70zQfDhT0w"
      },
      "outputs": [],
      "source": [
        "!mkdir -p nmt\n",
        "%cd nmt\n",
        "!git clone https://github.com/ymoslem/MT-Preparation.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8g6so3PXh8Kc"
      },
      "outputs": [],
      "source": [
        "# Install the requirements\n",
        "!pip3 install -r MT-Preparation/requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "US7HA0aciCeP"
      },
      "outputs": [],
      "source": [
        "%cd  ../"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_mdE7WYWjN1_"
      },
      "outputs": [],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j5UXPziljgVp"
      },
      "outputs": [],
      "source": [
        "!python3 nmt/MT-Preparation/subwording/3-desubword.py /content/drive/MyDrive/EngSwaBBC/target.model /content/drive/MyDrive/EngSwaBBC/GoURMETSwa_Translated_BestModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zHTXW0P3kw0K"
      },
      "outputs": [],
      "source": [
        "!python3 nmt/MT-Preparation/subwording/3-desubword.py /content/drive/MyDrive/EngSwaBBC/target.model /content/drive/MyDrive/EngSwaBBC/cleanGoURMETSwa.txt.subword.test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5zcDtaHujn3P"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Download the BLEU script\n",
        "!wget https://raw.githubusercontent.com/ymoslem/MT-Evaluation/main/BLEU/compute-bleu.py\n",
        "!pip3 install sacrebleu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cys-wvvakOmQ"
      },
      "outputs": [],
      "source": [
        "!python3 compute-bleu.py /content/drive/MyDrive/EngSwaBBC/cleanGoURMETSwa.txt.subword.test.desubword /content/drive/MyDrive/EngSwaBBC/GoURMETSwa_Translated_BestModel.desubword"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# METEOR Score Calculation"
      ],
      "metadata": {
        "id": "ga0QKw0GAQfo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.meteor_score import meteor_score, single_meteor_score\n",
        "import nltk"
      ],
      "metadata": {
        "id": "S0V0_REIAO-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download necessary NLTK data\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Function to read the content of a text file\n",
        "def read_file(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        return file.read()\n",
        "\n",
        "# Tokenize the text\n",
        "def tokenize(text):\n",
        "    return nltk.word_tokenize(text)"
      ],
      "metadata": {
        "id": "lASqIhQ2AVX8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to read the content of a text file\n",
        "def read_file(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        return file.read()\n",
        "\n",
        "# Paths to your text files\n",
        "file1_path = '/content/drive/MyDrive/EngSwaBBC/cleanGoURMETSwa.txt.subword.test.desubword'\n",
        "file2_path = '/content/drive/MyDrive/EngSwaBBC/GoURMETSwa_Translated_BestModel.desubword'\n",
        "\n",
        "# Read the contents of the files\n",
        "reference = read_file(file1_path)\n",
        "hypothesis = read_file(file2_path)\n",
        "\n",
        "# Tokenize the texts\n",
        "reference_tokens = tokenize(reference)\n",
        "hypothesis_tokens = tokenize(hypothesis)\n",
        "\n",
        "# Calculate the METEOR score\n",
        "score = meteor_score([reference_tokens], hypothesis_tokens)\n",
        "\n",
        "print(f'The METEOR score between the two files is: {score}')"
      ],
      "metadata": {
        "id": "FI3w7OmTAeWB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0C1zRojARqc3"
      },
      "outputs": [],
      "source": [
        "cat /content/drive/MyDrive/EngSwaBBC/GoURMETSwa_Translated_V3.desubword | sacrebleu /content/drive/MyDrive/EngSwaBBC/cleanGoURMETSwa.txt.subword.test.desubword -m bleu  -b -w 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2p9HsIvpmIve"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}